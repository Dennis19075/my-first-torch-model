{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196733a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git\n",
    "!pip install torchaudio\n",
    "!pip install torchcodec==0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8069dfb7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"base.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3b255f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "# Load an audio file (replace 'path_to_audio.wav' with your actual file path)\n",
    "audio_path = './example.m4a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94959780",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Make sure to preprocess the audio if necessary\n",
    "# For example, Whisper expects the audio to be in the correct format\n",
    "result = model.transcribe(audio_path)\n",
    "\n",
    "# Print the transcription\n",
    "print(result['text'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
